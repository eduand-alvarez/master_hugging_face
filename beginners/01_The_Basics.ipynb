{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37e992d3-79d0-459a-9629-ba76982e69fb",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this notebook, we'll introduce you to the world of Hugging Face. We'll cover the basics of both the Transformers and Datasets libraries. Hugging Face has become the go-to library for state-of-the-art Natural Language Processing (NLP) tasks, offering a wide variety of pre-trained models and datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1ef209-6cb4-46ba-8f41-bfb72b64dd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Up\n",
    "# First, let's install the required libraries:\n",
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4045377-52c9-41c3-b1b7-8c09ec04ff65",
   "metadata": {},
   "source": [
    "###  Transformers\n",
    "Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction, and text generation. Let's explore its basic functionalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd588a83-abe8-41a3-accb-483eee549b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading a Pre-trained Model\n",
    "# We'll start by loading the BERT model, a popular transformer model:\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Load pre-trained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13e1bd2-6b27-4feb-8b4b-38a19180c24c",
   "metadata": {},
   "source": [
    "###  Tokenization\n",
    "Before we can feed our text to the model, we need to convert it into tokens. This process is called tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b86d2a-c5a6-497d-a29d-3d3e83206c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hugging Face is creating transformative technology!\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fca2743-169a-4c46-9700-80bf97753778",
   "metadata": {},
   "source": [
    "# Model Inference\n",
    "Now, we can feed our tokenized input into the model to get embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dd9636-18ca-42d3-980a-84fdd86dacbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as torch\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(**encoded_input)\n",
    "    embeddings = output.last_hidden_state\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acf54dd-55f1-4b45-84ed-000547e40ed5",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "Hugging Face also provides a library called datasets which makes it easy to access a large number of datasets used in NLP research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fc5e39-98ac-41c7-af10-67eae625e591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading a Dataset\n",
    "# For demonstration purposes, we'll load the imdb dataset, a popular dataset for sentiment analysis:\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the IMDB dataset\n",
    "dataset = load_dataset(\"imdb\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec5c2b5-5d59-473d-be4a-7063dbde30f4",
   "metadata": {},
   "source": [
    "## Exploring the Dataset\n",
    "Datasets in the datasets library are often split into 'train', 'test', and sometimes 'validation' subsets. Let's take a peek at the first few entries of the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31aa3573-0e7a-487c-8092-8476c3ffcbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset['train'][0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42352de-4176-4a7f-9e72-e8d407f13f07",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Congratulations! You've just had a brief introduction to Hugging Face's Transformers and Datasets. There's a lot more to explore, including fine-tuning models on custom datasets, leveraging community-contributed models, and more. We encourage you to dive deeper into the documentation and community forums to continue your learning journey.\n",
    "\n",
    "Remember, the strength of Hugging Face lies not just in its powerful tools, but also in its vibrant community. Don't hesitate to share your projects and ask questions!\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

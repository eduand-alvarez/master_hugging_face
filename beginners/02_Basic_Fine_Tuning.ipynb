{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37e992d3-79d0-459a-9629-ba76982e69fb",
   "metadata": {},
   "source": [
    "# Fine-Tuning Transformers with Hugging Face\n",
    "In this notebook, we delve deeper into the Hugging Face ecosystem by exploring one of the most crucial tasks: fine-tuning pre-trained models on custom datasets.\n",
    "\n",
    "## Introduction\n",
    "Fine-tuning is the process of taking a pre-trained model (a model trained on a large dataset) and refining it on a smaller, specific dataset. This enables us to leverage the power of large-scale models like BERT or GPT-2 for our specific tasks without training from scratch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34965615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Up\n",
    "# Ensure you have the required libraries installed:\n",
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install torch\n",
    "!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2aacf3c",
   "metadata": {},
   "source": [
    "## Loading a Dataset\n",
    "For this demonstration, we'll use the imdb dataset. However, the process we'll follow is applicable to any dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76841a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "imdb = load_dataset(\"imdb\")\n",
    "print(imdb['train'][0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fa964a",
   "metadata": {},
   "source": [
    "## Preprocessing the Data\n",
    "Before fine-tuning, we need to preprocess our data into a format suitable for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9579d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def encode(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=256)\n",
    "\n",
    "encoded_imdb = imdb.map(encode, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bb0959",
   "metadata": {},
   "source": [
    "## Loading a Pre-trained Model\n",
    "We'll use the BertForSequenceClassification model, a BERT model fine-tuned for sequence classification tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43121ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3792a42",
   "metadata": {},
   "source": [
    "## Fine-Tuning the Model\n",
    "Now, we're all set to fine-tune our model on the IMDB dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398c2116",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=2,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=500,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    output_dir=\"./results\",\n",
    "    overwrite_output_dir=True,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_imdb[\"train\"],\n",
    "    eval_dataset=encoded_imdb[\"test\"],\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d144850",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating the Model\n",
    "#Let's assess the performance of our fine-tuned model:\n",
    "\n",
    "results = trainer.evaluate()\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0995e631",
   "metadata": {},
   "source": [
    "## Inference with the Fine-Tuned Model\n",
    "Now that we have a fine-tuned model, we can use it to predict the sentiment of new sentences. Let's see how to do this.\n",
    "\n",
    "### Tokenizing New Data\n",
    "First, let's create some new example sentences and tokenize them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855535e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"I absolutely loved that movie. It was fantastic!\",\n",
    "    \"The film was too long and quite boring.\",\n",
    "    \"The direction and acting were mediocre at best.\"\n",
    "]\n",
    "\n",
    "encoded_sentences = tokenizer(sentences, truncation=True, padding='max_length', max_length=256, return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da651ca",
   "metadata": {},
   "source": [
    "## Making Predictions\n",
    "Using our model, we can now make predictions on the tokenized sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7584328f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(**encoded_sentences).logits\n",
    "\n",
    "predictions = torch.argmax(logits, dim=1)\n",
    "sentiments = [\"Positive\" if pred == 1 else \"Negative\" for pred in predictions]\n",
    "\n",
    "for sentence, sentiment in zip(sentences, sentiments):\n",
    "    print(f\"'{sentence}' has a {sentiment} sentiment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40395d87",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "You've just fine-tuned a transformer model on a custom dataset! This process is at the heart of many NLP applications, allowing developers to harness the power of state-of-the-art models for specific tasks. Dive deeper, experiment with different models, and datasets, and unlock the full potential of transformers in your applications.\n",
    "\n",
    "You can expand the notebook by exploring hyperparameter tuning, different architectures, and other advanced topics. Remember to provide explanations and comments alongside the code to make it more instructional."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48937826",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
